{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "afe7d14f-448a-44fe-9ebc-a6cbb7308b5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.transformer import Transformer\n",
    "import boto3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09580e91-314b-44dc-b19f-132d10d8fe2a",
   "metadata": {},
   "source": [
    "SETTINGS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5427e885-749c-4a30-8f93-8606189ea828",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the variables (replace these with your actual values)\n",
    "model_name =\"my-logistic-model-from-training-job-new\" # The name of your existing model\n",
    "transform_job_name = 'my-fancy-name-logistic'\n",
    "input_data_path = 's3://sagemaker-bucket-ds/training-jobs/data/inference_input/'\n",
    "output_data_path = 's3://sagemaker-bucket-ds/training-jobs/data/inference_output/'\n",
    "instance_type = 'ml.m5.large'  # Replace with your preferred instance type"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc662278-f4c7-4e0b-be3a-c461999d6c31",
   "metadata": {},
   "source": [
    "DELETE THE OUTPUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1b327727-ac86-4d62-b949-f7dd3403a8e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 rm s3://sagemaker-bucket-ds/training-jobs/data/inference_output/ --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c983cac-cb5c-4ec9-8b16-43c471dca5e2",
   "metadata": {},
   "source": [
    "BASIC OBJECTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8f0d65db-254d-4b48-be9f-3b0e7d4224bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()  # You can also specify the role ARN directly\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bf0ab7-9356-456b-8819-a26b33ac66f1",
   "metadata": {},
   "source": [
    "CREATE TRANSFORMER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "73967918-a443-4cf2-9aff-56defd632a46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the Transformer object directly using the existing model name\n",
    "transformer = Transformer(\n",
    "    model_name=model_name,\n",
    "    instance_count=1,\n",
    "    instance_type=instance_type,\n",
    "    output_path=output_data_path,\n",
    "    assemble_with='Line',\n",
    "    accept='text/csv'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631e637f-a26d-44e2-a740-2d64616c7a14",
   "metadata": {},
   "source": [
    "MAKE TRANSFORMATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "68370b8c-0ae1-4f2c-be93-e3613a47babb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: sagemaker-scikit-learn-2024-07-01-15-14-52-041\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "...............................\u001b[34m2024-07-01 15:20:02,884 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:20:02,888 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:20:02,889 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2024-07-01 15:20:02,896 INFO - sagemaker-containers - Module inference does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2024-07-01 15:20:02,896 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2024-07-01 15:20:02,896 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2024-07-01 15:20:02,897 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: inference\n",
      "  Building wheel for inference (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for inference (setup.py): finished with status 'done'\n",
      "  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=3719 sha256=4c6f7bf5e0b698fd4b29c00402f19dd19af5b572f6162d5906238b5aa5b46f78\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-3_ik1l2z/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\u001b[0m\n",
      "\u001b[34mSuccessfully built inference\u001b[0m\n",
      "\u001b[34mInstalling collected packages: inference\u001b[0m\n",
      "\u001b[34mSuccessfully installed inference-1.0.0\u001b[0m\n",
      "\u001b[34m[2024-07-01 15:20:04 +0000] [24] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[34m[2024-07-01 15:20:04 +0000] [24] [INFO] Listening at: unix:/tmp/gunicorn.sock (24)\u001b[0m\n",
      "\u001b[34m[2024-07-01 15:20:04 +0000] [24] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2024-07-01 15:20:04 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[34m[2024-07-01 15:20:04 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[34m2024-07-01 15:20:11,623 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2024-07-01 15:20:11,623 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Jul/2024:15:20:12 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Jul/2024:15:20:12 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Jul/2024:15:20:12 +0000] \"POST /invocations HTTP/1.1\" 200 1303 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Jul/2024:15:20:12 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Jul/2024:15:20:12 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Jul/2024:15:20:12 +0000] \"POST /invocations HTTP/1.1\" 200 1303 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2024-07-01T15:20:12.091:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n",
      "\u001b[34m2024-07-01 15:20:02,884 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:20:02,888 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2024-07-01 15:20:02,889 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[35m2024-07-01 15:20:02,884 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2024-07-01 15:20:02,888 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2024-07-01 15:20:02,889 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2024-07-01 15:20:02,896 INFO - sagemaker-containers - Module inference does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2024-07-01 15:20:02,896 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2024-07-01 15:20:02,896 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2024-07-01 15:20:02,897 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: inference\n",
      "  Building wheel for inference (setup.py): started\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m2024-07-01 15:20:02,896 INFO - sagemaker-containers - Module inference does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m2024-07-01 15:20:02,896 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2024-07-01 15:20:02,896 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2024-07-01 15:20:02,897 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/miniconda3/bin/python -m pip install . \u001b[0m\n",
      "\u001b[35mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: inference\n",
      "  Building wheel for inference (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for inference (setup.py): finished with status 'done'\n",
      "  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=3719 sha256=4c6f7bf5e0b698fd4b29c00402f19dd19af5b572f6162d5906238b5aa5b46f78\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-3_ik1l2z/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\u001b[0m\n",
      "\u001b[34mSuccessfully built inference\u001b[0m\n",
      "\u001b[34mInstalling collected packages: inference\u001b[0m\n",
      "\u001b[34mSuccessfully installed inference-1.0.0\u001b[0m\n",
      "\u001b[35m  Building wheel for inference (setup.py): finished with status 'done'\n",
      "  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=3719 sha256=4c6f7bf5e0b698fd4b29c00402f19dd19af5b572f6162d5906238b5aa5b46f78\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-3_ik1l2z/wheels/f3/75/57/158162e9eab7af12b5c338c279b3a81f103b89d74eeb911c00\u001b[0m\n",
      "\u001b[35mSuccessfully built inference\u001b[0m\n",
      "\u001b[35mInstalling collected packages: inference\u001b[0m\n",
      "\u001b[35mSuccessfully installed inference-1.0.0\u001b[0m\n",
      "\u001b[34m[2024-07-01 15:20:04 +0000] [24] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[34m[2024-07-01 15:20:04 +0000] [24] [INFO] Listening at: unix:/tmp/gunicorn.sock (24)\u001b[0m\n",
      "\u001b[34m[2024-07-01 15:20:04 +0000] [24] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2024-07-01 15:20:04 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[34m[2024-07-01 15:20:04 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[35m[2024-07-01 15:20:04 +0000] [24] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[35m[2024-07-01 15:20:04 +0000] [24] [INFO] Listening at: unix:/tmp/gunicorn.sock (24)\u001b[0m\n",
      "\u001b[35m[2024-07-01 15:20:04 +0000] [24] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2024-07-01 15:20:04 +0000] [26] [INFO] Booting worker with pid: 26\u001b[0m\n",
      "\u001b[35m[2024-07-01 15:20:04 +0000] [27] [INFO] Booting worker with pid: 27\u001b[0m\n",
      "\u001b[34m2024-07-01 15:20:11,623 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2024-07-01 15:20:11,623 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Jul/2024:15:20:12 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Jul/2024:15:20:12 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [01/Jul/2024:15:20:12 +0000] \"POST /invocations HTTP/1.1\" 200 1303 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Jul/2024:15:20:12 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Jul/2024:15:20:12 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [01/Jul/2024:15:20:12 +0000] \"POST /invocations HTTP/1.1\" 200 1303 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2024-07-01T15:20:12.091:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "Transform job completed. Output stored in: s3://sagemaker-bucket-ds/training-jobs/data/inference_output/\n"
     ]
    }
   ],
   "source": [
    "# Start the batch transform job\n",
    "transformer.transform(\n",
    "    data=input_data_path,\n",
    "    content_type='text/csv',\n",
    "    split_type='Line'\n",
    ")\n",
    "\n",
    "# Wait for the transform job to complete\n",
    "transformer.wait()\n",
    "\n",
    "print(\"Transform job completed. Output stored in:\", output_data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae390394-efec-4b27-995b-fb7d78693b8a",
   "metadata": {},
   "source": [
    "MAKE TRANSFORMATION"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
