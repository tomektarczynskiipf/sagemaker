{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0d59e790-c678-4d40-b9ea-e55a9c8e6911",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sagemaker\n",
    "\n",
    "from sagemaker.xgboost import XGBoost\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "435769e6-67fd-46d9-8974-073c709d36f5",
   "metadata": {},
   "source": [
    "CREATE FOLDER TO SAVE TRAINING CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "85da02f2-a4cd-44a2-90a5-135a031ccf2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"training_code_04\", exist_ok=True) # Create folder for training code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c481bc5b-d7f2-4933-adf6-c8ec732e7199",
   "metadata": {},
   "source": [
    "CREATE TRAINING ENTRY POINT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14f25c37-9282-4aa9-8138-b385f57f778a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting training_code_04/train.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile training_code_04/train.py\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import joblib\n",
    "import os\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    model_dir = os.environ['SM_MODEL_DIR'] # Folder where model must be saved\n",
    "    train_dir = os.environ['SM_CHANNEL_TRAIN'] # Folder where train data is stored\n",
    "\n",
    "    # Lets assume there is only one training file\n",
    "    train_file_name = os.listdir(train_dir)[0]\n",
    "    train_file_path = os.path.join(train_dir, train_file_name)\n",
    "    \n",
    "    train_data = pd.read_csv(train_file_path, header=None, engine=\"python\")\n",
    "\n",
    "    # labels are in the first column\n",
    "    train_y = train_data.iloc[:, 0]\n",
    "    train_X = train_data.iloc[:, 1:]  \n",
    "\n",
    "    # Train the model\n",
    "    # Hyperparameters are hardcoded\n",
    "    clf = xgb.XGBClassifier(max_depth=5, n_estimators=100, learning_rate=0.1)\n",
    "    clf = clf.fit(train_X, train_y)\n",
    "\n",
    "    # Save model object\n",
    "    joblib.dump(clf, os.path.join(model_dir, \"model.joblib\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78338f04-4ecc-4df6-bb0d-ffb3c30225e3",
   "metadata": {},
   "source": [
    "CREATE NECCESSARY OBJECTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67e95f21-c120-44e9-9e13-e4f5f5fdd3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "role = get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82e4fb5-3edf-41d4-8c1d-0626d11a285e",
   "metadata": {},
   "source": [
    "CREATE ESTIMATOR OBJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "79869643-dd11-4e48-92a3-3b13e79e989c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py3.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.m5.large.\n"
     ]
    }
   ],
   "source": [
    "xgboost = XGBoost(\n",
    "    entry_point='train.py', # The file with the training code\n",
    "    source_dir='training_code_04', # The folder with the training code\n",
    "    framework_version='1.3-1', # Version of XGBoost which will be used\n",
    "    instance_type='ml.m5.large', # Instance type that will be used\n",
    "    instance_count=1, # number of instances to train\n",
    "    role=role, # Role that will be used during execution\n",
    "    sagemaker_session=sagemaker_session, \n",
    "    base_job_name='training-job-xgboost' # Name of the training job. Timestamp will be added as suffix\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d419ad-f917-47c7-bb44-dfe1e5653870",
   "metadata": {},
   "source": [
    "FIT THE MODEL\n",
    "- The fit function have one required parameter in the form of a dictionary.\n",
    "- The key represents the \"channel\" of the data. Typically it is train, test, valid. The names can be arbitrary.\n",
    "- The value contains path to S3 folder that contains the data\n",
    "\n",
    "The data from all channels will be copied to the training instance.\n",
    "\n",
    "To obtain LOCAL path to the data one should use os.environ['SM_CHANNEL_CHANNEL'] and replace CHANNEL by actual channel name\n",
    "\n",
    "Example:\n",
    "\n",
    "If we specify following dictionary {\"train\": \"s3://sagemaker-bucket-ds/training-jobs/data/train/\"} then in the training script the path to the training data is in the object os.environ['SM_CHANNEL_TRAIN']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "797a1aaf-ba39-447a-9f2c-5121ecf43571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating training-job with name: training-job-xgboost-2024-06-27-07-57-48-049\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-27 07:57:48 Starting - Starting the training job...\n",
      "2024-06-27 07:58:03 Starting - Preparing the instances for training...\n",
      "2024-06-27 07:58:34 Downloading - Downloading input data...\n",
      "2024-06-27 07:59:14 Downloading - Downloading the training image.....\u001b[34m[2024-06-27 07:59:59.702 ip-10-0-146-110.eu-west-1.compute.internal:7 INFO utils.py:28] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2024-06-27 07:59:59.730 ip-10-0-146-110.eu-west-1.compute.internal:7 INFO profiler_config_parser.py:111] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2024-06-27:07:59:59:INFO] Imported framework sagemaker_xgboost_container.training\u001b[0m\n",
      "\u001b[34m[2024-06-27:07:59:59:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-06-27:07:59:59:INFO] Invoking user training script.\u001b[0m\n",
      "\u001b[34m[2024-06-27:07:59:59:INFO] Module train does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m[2024-06-27:07:59:59:INFO] Generating setup.cfg\u001b[0m\n",
      "\u001b[34m[2024-06-27:07:59:59:INFO] Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m[2024-06-27:07:59:59:INFO] Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\u001b[0m\n",
      "\u001b[34m  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: train\n",
      "  Building wheel for train (setup.py): started\n",
      "  Building wheel for train (setup.py): finished with status 'done'\n",
      "  Created wheel for train: filename=train-1.0.0-py2.py3-none-any.whl size=3556 sha256=36ce321bfaec3565d2475ea43e6e34deafe10e7b1f71a079d01bb528df7b1492\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-_8fx0039/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built train\u001b[0m\n",
      "\u001b[34mInstalling collected packages: train\u001b[0m\n",
      "\u001b[34mSuccessfully installed train-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0.1 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[2024-06-27:08:00:01:INFO] No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m[2024-06-27:08:00:01:INFO] Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_xgboost_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"training-job-xgboost-2024-06-27-07-57-48-049\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-eu-west-1-211125740051/training-job-xgboost-2024-06-27-07-57-48-049/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 2,\n",
      "    \"num_gpus\": 0,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.m5.large\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.m5.large\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_xgboost_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=2\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=0\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-eu-west-1-211125740051/training-job-xgboost-2024-06-27-07-57-48-049/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_xgboost_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"training-job-xgboost-2024-06-27-07-57-48-049\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-eu-west-1-211125740051/training-job-xgboost-2024-06-27-07-57-48-049/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":2,\"num_gpus\":0,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.m5.large\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.m5.large\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/miniconda3/bin:/:/miniconda3/lib/python/site-packages/xgboost/dmlc-core/tracker:/miniconda3/lib/python37.zip:/miniconda3/lib/python3.7:/miniconda3/lib/python3.7/lib-dynload:/miniconda3/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m train\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\u001b[0m\n",
      "\u001b[34m[08:00:03] WARNING: ../src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'binary:logistic' was changed from 'error' to 'logloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\u001b[0m\n",
      "\n",
      "2024-06-27 08:00:18 Training - Training image download completed. Training in progress.\n",
      "2024-06-27 08:00:18 Uploading - Uploading generated training model\n",
      "2024-06-27 08:00:18 Completed - Training job completed\n",
      "Training seconds: 104\n",
      "Billable seconds: 104\n"
     ]
    }
   ],
   "source": [
    "xgboost.fit({\"train\": \"s3://sagemaker-bucket-ds/training-jobs/data/train/\"})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
